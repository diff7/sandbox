{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMasked(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_input_features, bias=True):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features : int\n",
    "        out_features : int\n",
    "        num_input_features : int\n",
    "            Number of features of the models input X.\n",
    "            These are needed for all masked layers.\n",
    "        bias : bool\n",
    "        \"\"\"\n",
    "        super(LinearMasked, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features, bias)\n",
    "        self.num_input_features = num_input_features\n",
    "\n",
    "        assert (\n",
    "            out_features >= num_input_features\n",
    "        ), \"To ensure autoregression, the output there should be enough hidden nodes. h >= in.\"\n",
    "\n",
    "        # Make sure that d-values are assigned to m\n",
    "        # d = 1, 2, ... D-1\n",
    "        d = set(range(1, num_input_features))\n",
    "        c = 0\n",
    "        while True:\n",
    "            c += 1\n",
    "            if c > 10:\n",
    "                break\n",
    "            # m function of the paper. Every hidden node, gets a number between 1 and D-1\n",
    "            self.m = torch.randint(1, num_input_features, size=(out_features,)).type(\n",
    "                torch.int32\n",
    "            )\n",
    "            if len(d - set(self.m.numpy())) == 0:\n",
    "                break\n",
    "\n",
    "            self.register_buffer(\n",
    "                \"mask\", torch.ones_like(self.linear.weight).type(torch.uint8)\n",
    "            )\n",
    "\n",
    "    def set_mask(self, m_previous_layer):\n",
    "        \"\"\"\n",
    "        Sets mask matrix of the current layer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        m_previous_layer : tensor\n",
    "            m values for previous layer layer.\n",
    "            The first layers should be incremental except for the last value,\n",
    "            as the model does not make a prediction P(x_D+1 | x_<D + 1).\n",
    "            The last prediction is P(x_D| x_<D)\n",
    "        \"\"\"\n",
    "        self.mask[...] = (m_previous_layer[:, None] <= self.m[None, :]).T\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.linear.bias is None:\n",
    "            b = 0\n",
    "        else:\n",
    "            b = self.linear.bias\n",
    "\n",
    "        return F.linear(x, self.linear.weight * self.mask, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_masked = LinearMasked(5, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
